{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用训练模型做预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import pickle\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sen = '你人真好'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取停用词\n",
    "def StopWords():\n",
    "    with open('train_data/stopwords.txt','r',encoding = 'utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    stopWords = []\n",
    "    for line in lines:\n",
    "        stopWords.append(line)\n",
    "    return stopWords\n",
    "# 读取训练数据，并去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读取训练数据，并去除停用词\n",
    "def read_data(input_sen):\n",
    "    sentence = []\n",
    "    trans = list(jieba.cut(input_sen.strip(), cut_all=False))\n",
    "\n",
    "    for word in trans:\n",
    "        if word not in stopWords:\n",
    "            sentence.append(word)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWords = StopWords()\n",
    "word2vec_path = 'word2vec/word2vec.pkl' # 词向量地址\n",
    "file = open(word2vec_path, 'rb')\n",
    "word2vec_model = pickle.load(file )\n",
    "vector_size=word2vec_model.vector_size # 词向量的维度\n",
    "# 将输入数据处理成向量矩阵\n",
    "MAX_SIZE=25 # 句子的统一长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def words2Array(line):\n",
    "    linesArray = []\n",
    "    wordsArray = []\n",
    "    steps = []\n",
    "\n",
    "    t = 0\n",
    "    p = 0\n",
    "    for i in range(MAX_SIZE):\n",
    "        # 如果小于MAX_SIZE 则用0.0的矩阵补齐\n",
    "        if i < len(line):\n",
    "            try:\n",
    "                wordsArray.append(word2vec_model.wv.word_vec(line[i]))\n",
    "                p = p + 1\n",
    "            except KeyError:\n",
    "                    # 如果在词向量模型中没有找到对应词的向量\n",
    "                t = t + 1\n",
    "                continue\n",
    "        else:\n",
    "            wordsArray.append(np.array([0.0] * vector_size))\n",
    "\n",
    "    for i in range(t):\n",
    "        wordsArray.append(np.array([0.0] * vector_size))\n",
    "    steps.append(p)\n",
    "    linesArray.append(wordsArray)\n",
    "\n",
    "    linesArray = np.array(linesArray)\n",
    "    steps = np.array(steps)\n",
    "    return linesArray, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 分词和去除停用词\n",
    "imput_data = read_data(input_sen)\n",
    "\n",
    "# 构造好输入数据\n",
    "data, steps = words2Array(imput_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model-senti\n",
      "[[ 0.28030765  0.71969241]]\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 128  #\n",
    "batch_size = 1 # 一次喂给训练器的数据条数\n",
    "output_size = 2 # 输出的维度\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.import_meta_graph('model/model-senti.meta')\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(\"model/\"))\n",
    "    prediction = tf.get_collection('pred_network')[0]\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "\n",
    "    input_x = graph.get_operation_by_name('input_x').outputs[0]\n",
    "    input_steps = graph.get_operation_by_name('steps').outputs[0]\n",
    "\n",
    "    pre = sess.run(prediction, feed_dict={input_x:data, input_steps: steps})\n",
    "    print(pre)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关系生拉硬拽。朋友们知道火箭助推器吗？就是推动火箭达到预定速度，之后就剥离脱落沙扬娜拉的东西。这片里所有除了男主以外的正面角色，基本都是火箭助推器。皮卡丘自带超越主角的光环不计算在内。但这事儿主要怪男主，牵着不走打着倒退。情感转折突如其来。编剧说他们是好伙伴就是，咱也不知道，咱也不敢问。大难临头分道扬镳，危机散尽相视莫逆。资本主义没有真爱。\n",
      "句子的情感极性：消极\n"
     ]
    }
   ],
   "source": [
    "print(input_sen)\n",
    "if pre[0][0] > pre[0][1]:\n",
    "    print('句子的情感极性：积极')\n",
    "else:\n",
    "    print('句子的情感极性：消极')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
